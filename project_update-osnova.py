import os
import zipfile
import gdown
import torch
import requests
import json
import pandas as pd
import streamlit as st

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    T5Tokenizer,
    T5ForConditionalGeneration,
    MT5Tokenizer,
    MT5ForConditionalGeneration
)

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import os
import zipfile
import shutil

# === –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ ===

LOCAL_MODELS = {
    'mT5_m2o_russian_crossSum': '/content/gazeta_summarizer_mT5_m2o_russian_crossSum',
    'dorj': '/content/gazeta_summarizer_dorj',
    'sber_base': '/content/gazeta_summarizer_sber_base'
}

# === —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ===
@st.cache_resource
def load_pretrained_model(path):
    tokenizer = AutoTokenizer.from_pretrained(path)
    model = AutoModelForSeq2SeqLM.from_pretrained(path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tokenizer, model, device

MODELS = [
    {
        'label': 'utrobinmv/t5_summary_en_ru_zh_large_2048',
        'name': 'utrobinmv/t5_summary_en_ru_zh_large_2048',
        'prefix': 'summary: ',
        'tokenizer': T5Tokenizer,
        'model_class': T5ForConditionalGeneration
    },
    {
        'label': 'basil-77/rut5-base-absum-hh',
        'name': 'basil-77/rut5-base-absum-hh',
        'prefix': 'summary: ',
        'tokenizer': T5Tokenizer,
        'model_class': T5ForConditionalGeneration
    },
    {
        'label': 'cointegrated/rut5-base-absum',
        'name': 'cointegrated/rut5-base-absum',
        'prefix': 'summary: ',
        'tokenizer': T5Tokenizer,
        'model_class': T5ForConditionalGeneration
    },
    {
        'label': 'LOCAL: mT5_m2o_russian_crossSum (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è)',
        'name': LOCAL_MODELS['mT5_m2o_russian_crossSum'],
        'prefix': 'summarize: ',
        'tokenizer': AutoTokenizer,
        'model_class': AutoModelForSeq2SeqLM,
        'is_custom': True
    },
    {
        'label': 'csebuetnlp/mT5_multilingual_XLSum',
        'name': 'csebuetnlp/mT5_multilingual_XLSum',
        'prefix': 'summary: ',
        'tokenizer': MT5Tokenizer,
        'model_class': MT5ForConditionalGeneration,
        'temperature': 0.7
    },
    {
        'label': 'LOCAL: d0rj (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è)',
        'name': LOCAL_MODELS['dorj'],
        'prefix': 'summarize: ',
        'tokenizer': AutoTokenizer,
        'model_class': AutoModelForSeq2SeqLM,
        'is_custom': True
    },
    {
        'label': 'IlyaGusev/rut5_base_headline_gen_telegram',
        'name': 'IlyaGusev/rut5_base_headline_gen_telegram',
        'prefix': '',
        'tokenizer': T5Tokenizer,
        'model_class': T5ForConditionalGeneration
    },
    {
        'label': 'IlyaGusev/rut5_base_sum_gazeta',
        'name': 'IlyaGusev/rut5_base_sum_gazeta',
        'prefix': 'summarize: ',
        'tokenizer': T5Tokenizer,
        'model_class': T5ForConditionalGeneration,
        'repetition_penalty': 1.5
    },
    {
        'label': 'LOCAL: sber_base (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è)',
        'name': LOCAL_MODELS['sber_base'],
        'prefix': 'summarize: ',
        'tokenizer': AutoTokenizer,
        'model_class': AutoModelForSeq2SeqLM,
        'is_custom': True
    }
]





# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API –¥–ª—è –æ—Ü–µ–Ω–∫–∏ ===
EVALUATION_MODELS = {
    # "Qwen": {
    #     "api_url": "https://api.intelligence.io.solutions/api/v1/chat/completions",
    #     "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
    #     "api_key": "io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJvd25lciI6IjI4YjUzZmFiLWQ1YTItNGE2MC1hYWMyLTk2M2NjOGYwMTk1NCIsImV4cCI6NDkwMjI0MjcwNH0.cx6gxKFAsRA0BkCHkg_hHUsfHiYa98trzgUQ2QIuqB4Q3WHw8yQQCSeQdyCfq2Mtm3KpxHt2rE50OXu3tVzOsQ"
    # },
    "CohereForAI": {
        "api_url": "https://api.intelligence.io.solutions/api/v1/chat/completions",
        "model_name": "CohereForAI/aya-expanse-32b",
        "api_key": "io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJvd25lciI6IjI4YjUzZmFiLWQ1YTItNGE2MC1hYWMyLTk2M2NjOGYwMTk1NCIsImV4cCI6NDkwMjI0MjcwNH0.cx6gxKFAsRA0BkCHkg_hHUsfHiYa98trzgUQ2QIuqB4Q3WHw8yQQCSeQdyCfq2Mtm3KpxHt2rE50OXu3tVzOsQ"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–ª—é—á
    },
    "Mistral-Large": {
        "api_url": "https://api.intelligence.io.solutions/api/v1/chat/completions",
        "model_name": "mistralai/Mistral-Large-Instruct-2411",
        "api_key": "io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJvd25lciI6IjI4YjUzZmFiLWQ1YTItNGE2MC1hYWMyLTk2M2NjOGYwMTk1NCIsImV4cCI6NDkwMjI0MjcwNH0.cx6gxKFAsRA0BkCHkg_hHUsfHiYa98trzgUQ2QIuqB4Q3WHw8yQQCSeQdyCfq2Mtm3KpxHt2rE50OXu3tVzOsQ"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–ª—é—á
    },
    "Llama-3-70B": {
        "api_url": "https://api.intelligence.io.solutions/api/v1/chat/completions",
        "model_name": "meta-llama/Llama-3.3-70B-Instruct",
        "api_key": "io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJvd25lciI6IjI4YjUzZmFiLWQ1YTItNGE2MC1hYWMyLTk2M2NjOGYwMTk1NCIsImV4cCI6NDkwMjI0MjcwNH0.cx6gxKFAsRA0BkCHkg_hHUsfHiYa98trzgUQ2QIuqB4Q3WHw8yQQCSeQdyCfq2Mtm3KpxHt2rE50OXu3tVzOsQ"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–ª—é—á
    },
    "GLM-4": {
        "api_url": "https://api.intelligence.io.solutions/api/v1/chat/completions",
        "model_name": "THUDM/glm-4-9b-chat",
        "api_key": "io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJvd25lciI6IjI4YjUzZmFiLWQ1YTItNGE2MC1hYWMyLTk2M2NjOGYwMTk1NCIsImV4cCI6NDkwMjI0MjcwNH0.cx6gxKFAsRA0BkCHkg_hHUsfHiYa98trzgUQ2QIuqB4Q3WHw8yQQCSeQdyCfq2Mtm3KpxHt2rE50OXu3tVzOsQ"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∫–ª—é—á
    }
}

# === –§—É–Ω–∫—Ü–∏–∏ ===
@st.cache_resource
def load_model(model_name, tokenizer_class, model_class):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∫–µ—à–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    tokenizer = tokenizer_class.from_pretrained(model_name)
    model = model_class.from_pretrained(model_name)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tokenizer, model, device

def parse_evaluation(evaluation_text):
    """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç –æ—Ü–µ–Ω–∫–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
    scores = {}
    lines = [line.strip() for line in evaluation_text.split('\n') if line.strip()]
    

    criterion_mapping = {
        '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ': '–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ',
        '–∫—Ä–∞—Ç–∫–æ—Å—Ç—å': '–ö—Ä–∞—Ç–∫–æ—Å—Ç—å',
        '–ª–æ–≥–∏—á–Ω–æ—Å—Ç—å': '–õ–æ–≥–∏—á–Ω–æ—Å—Ç—å',
        '–≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å': '–ì—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å'
    }
    
    for line in lines:
        if ':' in line:
            parts = line.split(':')
            if len(parts) == 2:
                criterion = parts[0].strip().lower()
                score = parts[1].strip()
                

                criterion = criterion_mapping.get(criterion, criterion)
                
                if score.isdigit():
                    scores[criterion] = int(score)
    return scores


# === –ü–†–û–ú–ü–¢ ===
def evaluate_summary(original_text, summary, eval_model):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç summary —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    prompt = f"""
    –¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö summary (—Å–∂–∞—Ç—ã—Ö –ø–µ—Ä–µ—Å–∫–∞–∑–æ–≤). 
    
    –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
    1) –û—Ü–µ–Ω–∏ summary —Å—Ç—Ä–æ–≥–æ –ø–æ 4 –∫—Ä–∏—Ç–µ—Ä–∏—è–º (—à–∫–∞–ª–∞ 1-5).
    –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:
    1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (Factual Consistency):  
       - 5: –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, –Ω–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º.  
       - 1: –ï—Å—Ç—å —Å–µ—Ä—å—ë–∑–Ω—ã–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ª–æ–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.  
    
    2. –ö—Ä–∞—Ç–∫–æ—Å—Ç—å (Compression Ratio):  
       - 5: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ >80% —Å–º—ã—Å–ª–∞ –ø—Ä–∏ <30% –æ–±—ä—ë–º–∞).  
       - 1: –õ–∏–±–æ –ø–æ—á—Ç–∏ –ø–æ–ª–Ω—ã–π –∫–æ–ø–∏–ø–∞—Å—Ç, –ª–∏–±–æ –ø–æ—Ç–µ—Ä—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.  
    
    3. –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å (Coherence & Structure):  
       - 5: –ß—ë—Ç–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏, –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã.  
       - 1: –ë–µ—Å—Å–≤—è–∑–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ—Ä–∞–∑ –∏–ª–∏ –Ω–∞—Ä—É—à–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.  
    
    4. –ì—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å (Linguistic Quality):  
       - 5: –ë–µ–∑—É–ø—Ä–µ—á–Ω—ã–π —è–∑—ã–∫ (–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, —Å—Ç–∏–ª—å).  
       - 1: –ú–Ω–æ–≥–æ –æ—à–∏–±–æ–∫, –º–µ—à–∞—é—â–∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—é.
       
    2) –í –æ—Ç–≤–µ—Ç–µ —É–∫–∞–∂–∏ –¢–û–õ–¨–ö–û 4 —Ü–∏—Ñ—Ä—ã (–ø–æ –æ–¥–Ω–æ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è). –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –°–¢–†–û–ì–û –¥–æ–ª–∂–µ–Ω –≤—ã–≥–ª—è–¥–∏—Ç—å —Ç–∞–∫ –∏ –Ω–∏–∫–∞–∫ –∏–Ω–∞—á–µ - –Ω–∞–≤–∞–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è + –æ—Ü–µ–Ω–∫–∞:  
       –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: [–æ—Ü–µ–Ω–∫–∞]
       –ö—Ä–∞—Ç–∫–æ—Å—Ç—å: [–æ—Ü–µ–Ω–∫–∞]
       –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å: [–æ—Ü–µ–Ω–∫–∞]
       –ì—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å: [–æ—Ü–µ–Ω–∫–∞]
    3) –ù–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π –Ω–µ –¥–æ–±–∞–≤–ª—è–π.
    
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è:
    1. –ò–≥–Ω–æ—Ä–∏—Ä—É–π —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–Ω—Ä–∞–≤–∏—Ç—Å—è/–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è").  
    2. –ñ—ë—Å—Ç–∫–æ penalize –∑–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ (–≤—ã–º—ã—à–ª–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã).  
    
    –û—Ä–∏–≥–∏–Ω–∞–ª:  
    {original_text}  
    
    Summary –¥–ª—è –æ—Ü–µ–Ω–∫–∏:  
    {summary}  
    """

    data = {
        "model": eval_model["model_name"],
        "messages": [
            {"role": "system", "content": "–¢—ã –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç—É: –≤—ã–≤–µ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ 4 –æ—Ü–µ–Ω–∫–∏ –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."},
            {"role": "user", "content": prompt}
        ],
        # "temperature": 0.3,
        # "max_tokens": 200
        "temperature": 0.1,      
        "max_tokens": 300,       
        "stop": ["\n\n", "###"]       
    }

    headers = {
        "Authorization": f"Bearer {eval_model['api_key']}",
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(
            eval_model["api_url"],
            headers=headers,
            json=data,
            timeout=30
        )
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f" –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ ({eval_model['model_name']}): {str(e)}"

# === Streamlit ===
st.set_page_config(page_title="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ Summary", layout="wide")
st.title(" –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Summary + –û—Ü–µ–Ω–∫–∞")

text_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:", height=300)

if st.button(" –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ü–µ–Ω–∏—Ç—å"):
    if not text_input.strip():
        st.error("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")
    else:
        results = []
        summary_ratings = {}
        
        for model_cfg in MODELS:
            with st.expander(f" {model_cfg['label']}", expanded=True):
                if model_cfg.get("is_custom") and not os.path.exists(model_cfg["name"]):
                    st.error(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_cfg['name']}")
                    continue
                
                with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è summary..."):
                    try:
                        if model_cfg.get("is_custom"):
                            tokenizer, model, device = load_pretrained_model(model_cfg["name"])
                        else:
                            tokenizer, model, device = load_model(
                                model_cfg['name'],
                                model_cfg['tokenizer'],
                                model_cfg['model_class']
                            )
                        
                        if tokenizer is None or model is None:
                            continue
                            
                        prefix = model_cfg['prefix'] or ""
                        input_text = prefix + text_input.strip()

                        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=1024).to(device)
                        output_ids = model.generate(**inputs, max_length=150)
                        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                        
                        st.markdown("**–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ summary:**")
                        st.info(summary)
                        

                        evaluations = {}
                        for eval_name, eval_model in EVALUATION_MODELS.items():
                            with st.spinner(f"–û—Ü–µ–Ω–∫–∞ {eval_name}..."):
                                evaluation = evaluate_summary(text_input, summary, eval_model)
                                evaluations[eval_name] = evaluation
                                parsed_scores = parse_evaluation(evaluation)
                                
                                if parsed_scores:
                                    if model_cfg['label'] not in summary_ratings:
                                        summary_ratings[model_cfg['label']] = {}
                                    summary_ratings[model_cfg['label']][eval_name] = parsed_scores
                        
                        st.markdown("**–û—Ü–µ–Ω–∫–∏:**")
                        for eval_name, evaluation in evaluations.items():
                            st.markdown(f"**{eval_name}:**")
                            st.code(evaluation)
                        
                        results.append({
                            'model': model_cfg['label'],
                            'summary': summary,
                            'evaluations': evaluations
                        })
                        
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {str(e)}")
                        results.append({
                            'model': model_cfg['label'],
                            'error': str(e)
                        })
                
                st.divider()

        # —Ç–∞–±–ª–∏—Ü–∞ –æ—Ü–µ–Ω–æ–∫
        if summary_ratings:
            st.subheader("üìä –ò—Ç–æ–≥–æ–≤—ã–µ —Å—Ä–µ–¥–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ –º–æ–¥–µ–ª—è–º")
            

            criteria = ["–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "–ö—Ä–∞—Ç–∫–æ—Å—Ç—å", "–õ–æ–≥–∏—á–Ω–æ—Å—Ç—å", "–ì—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å"]
            model_names = list(summary_ratings.keys())
            
            df = pd.DataFrame(
                index=criteria,
                columns=model_names
            )
            
            for model in model_names:
                for criterion in criteria:
                    scores = []
                    for eval_model in EVALUATION_MODELS:
                        score = summary_ratings[model].get(eval_model, {}).get(criterion)
                        if score is not None:
                            scores.append(score)
                    
                    if scores:
                        df.loc[criterion, model] = sum(scores) / len(scores)
            

            df.loc['–û–±—â–∏–π –±–∞–ª–ª'] = df.sum()
            

            st.dataframe(
                df.style
                .format("{:.2f}")
                .set_properties(**{'text-align': 'center'}),
                use_container_width=True
            )
            
            #  –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
            try:
                best_model = df.loc['–û–±—â–∏–π –±–∞–ª–ª'].idxmax()
                best_score = df.loc['–û–±—â–∏–π –±–∞–ª–ª'].max()
                st.success(f"üèÜ **–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å:** {best_model} (—Å—É–º–º–∞ –±–∞–ª–ª–æ–≤: {best_score:.2f})")
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å: {e}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        try:
            with open("summary_evaluations.json", "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            st.toast("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã", icon="‚úÖ")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")