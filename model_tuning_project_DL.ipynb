{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-k6tzZ0SyIQ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall \\\n",
        "  transformers==4.40.1 \\\n",
        "  peft==0.10.0 \\\n",
        "  accelerate==0.27.2 \\\n",
        "  datasets==2.18.0 \\\n",
        "  evaluate==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S42EQn9hJ3Dr"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install huggingface_hub[hf_xet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7JdhAXxFPZcw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import hf_xet\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    pipeline,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq, GenerationConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDh5C0EWRFif"
      },
      "source": [
        "Процесс дообучения моделей на датасете Gazeta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IUycvk_PZZX"
      },
      "outputs": [],
      "source": [
        "def load_gazeta_dataset(split=\"train\", num_samples=None, min_chars=100, max_chars=5000):\n",
        "\n",
        "    # Загружает датасет Gazeta с возможностью фильтрации по длине текста\n",
        "\n",
        "    # :param split: Раздел датасета (train/validation/test)\n",
        "    # :param num_samples: Ограничение количества примеров\n",
        "    # :param min_chars: Минимальная длина текста\n",
        "    # :param max_chars: Максимальная длина текста\n",
        "    # :return: Отфильтрованный датасет\n",
        "\n",
        "    print(f\"Загружаем Gazeta ({split})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"IlyaGusev/gazeta\", \"v1.5\", split=split)\n",
        "    except:\n",
        "        dataset = load_dataset(\"IlyaGusev/gazeta\", split=split)\n",
        "\n",
        "    # Фильтрация по длине текста\n",
        "    filtered = dataset.filter(lambda x: min_chars <= len(x[\"text\"]) <= max_chars)\n",
        "\n",
        "    if num_samples:\n",
        "        available_samples = len(filtered)\n",
        "        actual_samples = min(num_samples, available_samples)\n",
        "        print(f\"Запрошено {num_samples} примеров, доступно {available_samples}. Используем {actual_samples}.\")\n",
        "        return filtered.select(range(actual_samples))\n",
        "    return filtered\n",
        "\n",
        "class GazetaSummarizer:\n",
        "    def __init__(self, model_name=\"csebuetnlp/mT5_m2o_russian_crossSum\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        # конфигурация генерации\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=150,\n",
        "            num_beams=5,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_k=60,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.3,\n",
        "            length_penalty=1.2\n",
        "        )\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        # Функция токенизации с учетом датасета Gazeta\n",
        "        texts = examples[\"text\"]\n",
        "        summaries = examples[\"summary\"]\n",
        "\n",
        "        # Добавляем префикс для T5 моделей\n",
        "        if 't5' in self.model_name.lower():\n",
        "            texts = [f\"summarize: {text}\" for text in texts]\n",
        "\n",
        "        # Токенизация текстов\n",
        "        model_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            max_length=600,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Токенизация пересказов\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                summaries,\n",
        "                max_length=150,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "        model_inputs[\"labels\"] = labels\n",
        "        return model_inputs\n",
        "\n",
        "    def train(self, train_data, val_data=None, output_dir=\"./gazeta_summarizer\"):\n",
        "\n",
        "        # Обучение модели\n",
        "\n",
        "        # Разделение данных если не предоставлен валидационный набор\n",
        "        if val_data is None:\n",
        "            split = train_data.train_test_split(test_size=0.15)\n",
        "            train_data = split[\"train\"]\n",
        "            val_data = split[\"test\"]\n",
        "\n",
        "        # Подготовка данных\n",
        "        train_dataset = train_data.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=32,\n",
        "            remove_columns=train_data.column_names\n",
        "        )\n",
        "        val_dataset = val_data.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=32,\n",
        "            remove_columns=val_data.column_names\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(\n",
        "            self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=-100\n",
        "        )\n",
        "\n",
        "        # параметры обучения\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=1000,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=1000,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=7,  # количество эпох\n",
        "            learning_rate=3e-5,  # learning rate\n",
        "            weight_decay=0.01,\n",
        "            warmup_steps=800,\n",
        "            logging_dir=f\"{output_dir}/logs\",\n",
        "            logging_steps=200,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            predict_with_generate=True,\n",
        "            report_to=\"none\",\n",
        "            gradient_accumulation_steps=2,  # Для стабилизации обучения\n",
        "            lr_scheduler_type=\"cosine\",     # Косинусный планировщик\n",
        "            save_total_limit=2              # Лучше экономит место\n",
        "        )\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            data_collator=data_collator\n",
        "        )\n",
        "\n",
        "        print(\"\\n Начинаем обучение...\")\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        # Сохранение результатов\n",
        "        trainer.save_model(output_dir)\n",
        "        trainer.save_state()\n",
        "        trainer.save_metrics(\"train\", train_result.metrics)\n",
        "\n",
        "        # Детальная оценка после обучения\n",
        "        val_metrics = trainer.evaluate(\n",
        "            metric_key_prefix=\"final_eval\",\n",
        "            max_length=150,  # Соответствует генерации\n",
        "            num_beams=5\n",
        "        )\n",
        "        trainer.save_metrics(\"eval\", val_metrics)\n",
        "\n",
        "        print(f\"\\n Обучение завершено. Модель сохранена в {output_dir}\")\n",
        "        print(f\"Train loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "        print(f\"Eval loss: {val_metrics['final_eval_loss']:.4f}\")\n",
        "\n",
        "        return train_result.metrics\n",
        "\n",
        "    def generate_summary(self, text, max_length=150):\n",
        "        # Генерация пересказа с улучшенной постобработкой\n",
        "        if 't5' in self.model_name.lower():\n",
        "            text = f\"summarize: {text}\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=600,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                generation_config=self.generation_config,\n",
        "                max_length=max_length\n",
        "            )\n",
        "\n",
        "        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return self._postprocess_summary(summary)\n",
        "\n",
        "    def _postprocess_summary(self, text):\n",
        "        # Улучшенная постобработка текста\n",
        "\n",
        "        # Удаление служебных токенов и лишних пробелов\n",
        "        text = re.sub(r'<extra_id_\\d+>|\\[.*?\\]|(Пересказ|Сводка|Резюме):?\\s*', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Коррекция пунктуации\n",
        "        text = re.sub(r'\\s([,.!?])', r'\\1', text)\n",
        "        text = re.sub(r'([,.!?])([А-Яа-я])', r'\\1 \\2', text)\n",
        "\n",
        "        # Добавление точки в конце если нужно\n",
        "        if text and text[-1] not in {'.', '!', '?'}:\n",
        "            text += '.'\n",
        "\n",
        "        # Выбор первых 3 предложений (если нужно)\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        return ' '.join(sentences[:3]) if len(sentences) > 3 else text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Загрузка данных Gazeta\n",
        "    train_data = load_gazeta_dataset(split=\"train\", num_samples=5000)\n",
        "    val_data = load_gazeta_dataset(split=\"validation\")\n",
        "\n",
        "    # 2. Инициализация модели (можно попробовать large если есть ресурсы)\n",
        "    summarizer = GazetaSummarizer(model_name=\"csebuetnlp/mT5_m2o_russian_crossSum\")\n",
        "\n",
        "    # 3. Обучение\n",
        "    summarizer.train(\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        output_dir=\"./gazeta_summarizer\"\n",
        "    )\n",
        "\n",
        "    # 4. Тестирование на примерах\n",
        "    test_data = load_gazeta_dataset(split=\"test\", num_samples=5)\n",
        "    for i, example in enumerate(test_data):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"📰 Пример {i+1}\")\n",
        "        print(\"Оригинал:\", example[\"text\"][:200].replace(\"\\n\", \" \") + \"...\")\n",
        "        print(\"Эталон:\", example[\"summary\"])\n",
        "        print(\"Сгенерировано:\", summarizer.generate_summary(example[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfwuZg84SqQE"
      },
      "source": [
        "Тест на произвольном тексте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLNTmArTPZWf"
      },
      "outputs": [],
      "source": [
        "model_path = \"./gazeta_summarizer\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "def summarize(text):\n",
        "    inputs = tokenizer(\n",
        "        f\"summarize: {text}\",\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=600,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=150,\n",
        "            num_beams=5,\n",
        "            do_sample=True,\n",
        "            top_k=60,\n",
        "            top_p=0.9,\n",
        "            temperature=0.8,\n",
        "            repetition_penalty=1.3,\n",
        "            length_penalty=1.2\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD0iVCdqPZT5"
      },
      "outputs": [],
      "source": [
        "texts='Почти 1,3 тысячи жителей курского приграничья, которые числились в реестре утративших связь с родственниками из-за действий ВСУ, найдены на данный момент. Об этом сообщил врио губернатора региона Александр Хинштейн. Кроме того, по его словам, установлено примерное нахождение еще 421 человека. И цифра людей, чье местонахождение сегодня, увы, нам неизвестно, - на данную минуту это 576 человек, - добавил Хинштейн, отметив, что это очень большая цифра, но она корректная, поскольку сложилась из ряда источников. С момента создания реестра в него было внесено 2287 человек, уточнил врио губернатора региона. Он также пояснил, что при формировании соответствующего реестра собирались данные от целого ряда ведомств. Власти в том числе запрашивали в СФР данные о пенсионерах, которые с августа 2024 года не получали пенсию из приграничных районов.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPgQpbghPZQz"
      },
      "outputs": [],
      "source": [
        "print(summarize(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6UavJaMTI2c"
      },
      "source": [
        "zip модели, для последующей скачки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0VOg64UTOO8"
      },
      "outputs": [],
      "source": [
        "!zip -r gazeta_summarizer.zip ./gazeta_summarizer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
