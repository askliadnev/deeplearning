{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-k6tzZ0SyIQ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall \\\n",
        "  transformers==4.40.1 \\\n",
        "  peft==0.10.0 \\\n",
        "  accelerate==0.27.2 \\\n",
        "  datasets==2.18.0 \\\n",
        "  evaluate==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S42EQn9hJ3Dr"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install huggingface_hub[hf_xet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7JdhAXxFPZcw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import hf_xet\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    pipeline,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq, GenerationConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import gc\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDh5C0EWRFif"
      },
      "source": [
        "–ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Gazeta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IUycvk_PZZX"
      },
      "outputs": [],
      "source": [
        "def load_gazeta_dataset(split=\"train\", num_samples=None, min_chars=100, max_chars=5000):\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç Gazeta —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "    # :param split: –†–∞–∑–¥–µ–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (train/validation/test)\n",
        "    # :param num_samples: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "    # :param min_chars: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "    # :param max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "    # :return: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "\n",
        "    print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º Gazeta ({split})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"IlyaGusev/gazeta\", \"v1.5\", split=split)\n",
        "    except:\n",
        "        dataset = load_dataset(\"IlyaGusev/gazeta\", split=split)\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞\n",
        "    filtered = dataset.filter(lambda x: min_chars <= len(x[\"text\"]) <= max_chars)\n",
        "\n",
        "    if num_samples:\n",
        "        available_samples = len(filtered)\n",
        "        actual_samples = min(num_samples, available_samples)\n",
        "        print(f\"–ó–∞–ø—Ä–æ—à–µ–Ω–æ {num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤, –¥–æ—Å—Ç—É–ø–Ω–æ {available_samples}. –ò—Å–ø–æ–ª—å–∑—É–µ–º {actual_samples}.\")\n",
        "        return filtered.select(range(actual_samples))\n",
        "    return filtered\n",
        "\n",
        "class GazetaSummarizer:\n",
        "    def __init__(self, model_name=\"csebuetnlp/mT5_m2o_russian_crossSum\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        # –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=150,\n",
        "            num_beams=5,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_k=60,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.3,\n",
        "            length_penalty=1.2\n",
        "        )\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        # –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–∞ Gazeta\n",
        "        texts = examples[\"text\"]\n",
        "        summaries = examples[\"summary\"]\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è T5 –º–æ–¥–µ–ª–µ–π\n",
        "        if 't5' in self.model_name.lower():\n",
        "            texts = [f\"summarize: {text}\" for text in texts]\n",
        "\n",
        "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
        "        model_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            max_length=600,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ—Å–∫–∞–∑–æ–≤\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = self.tokenizer(\n",
        "                summaries,\n",
        "                max_length=150,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "        model_inputs[\"labels\"] = labels\n",
        "        return model_inputs\n",
        "\n",
        "    def train(self, train_data, val_data=None, output_dir=\"./gazeta_summarizer\"):\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä\n",
        "        if val_data is None:\n",
        "            split = train_data.train_test_split(test_size=0.15)\n",
        "            train_data = split[\"train\"]\n",
        "            val_data = split[\"test\"]\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        train_dataset = train_data.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=32,\n",
        "            remove_columns=train_data.column_names\n",
        "        )\n",
        "        val_dataset = val_data.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=32,\n",
        "            remove_columns=val_data.column_names\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(\n",
        "            self.tokenizer,\n",
        "            model=self.model,\n",
        "            label_pad_token_id=-100\n",
        "        )\n",
        "\n",
        "        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=1000,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=1000,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=7,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
        "            learning_rate=3e-5,  # learning rate\n",
        "            weight_decay=0.01,\n",
        "            warmup_steps=800,\n",
        "            logging_dir=f\"{output_dir}/logs\",\n",
        "            logging_steps=200,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            predict_with_generate=True,\n",
        "            report_to=\"none\",\n",
        "            gradient_accumulation_steps=2,  # –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
        "            lr_scheduler_type=\"cosine\",     # –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫\n",
        "            save_total_limit=2              # –õ—É—á—à–µ —ç–∫–æ–Ω–æ–º–∏—Ç –º–µ—Å—Ç–æ\n",
        "        )\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            data_collator=data_collator\n",
        "        )\n",
        "\n",
        "        print(\"\\n –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        trainer.save_model(output_dir)\n",
        "        trainer.save_state()\n",
        "        trainer.save_metrics(\"train\", train_result.metrics)\n",
        "\n",
        "        # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "        val_metrics = trainer.evaluate(\n",
        "            metric_key_prefix=\"final_eval\",\n",
        "            max_length=150,  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "            num_beams=5\n",
        "        )\n",
        "        trainer.save_metrics(\"eval\", val_metrics)\n",
        "\n",
        "        print(f\"\\n –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}\")\n",
        "        print(f\"Train loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "        print(f\"Eval loss: {val_metrics['final_eval_loss']:.4f}\")\n",
        "\n",
        "        return train_result.metrics\n",
        "\n",
        "    def generate_summary(self, text, max_length=150):\n",
        "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ—Å–∫–∞–∑–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–æ–π\n",
        "        if 't5' in self.model_name.lower():\n",
        "            text = f\"summarize: {text}\"\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=600,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                generation_config=self.generation_config,\n",
        "                max_length=max_length\n",
        "            )\n",
        "\n",
        "        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return self._postprocess_summary(summary)\n",
        "\n",
        "    def _postprocess_summary(self, text):\n",
        "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "        text = re.sub(r'<extra_id_\\d+>|\\[.*?\\]|(–ü–µ—Ä–µ—Å–∫–∞–∑|–°–≤–æ–¥–∫–∞|–†–µ–∑—é–º–µ):?\\s*', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
        "        text = re.sub(r'\\s([,.!?])', r'\\1', text)\n",
        "        text = re.sub(r'([,.!?])([–ê-–Ø–∞-—è])', r'\\1 \\2', text)\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "        if text and text[-1] not in {'.', '!', '?'}:\n",
        "            text += '.'\n",
        "\n",
        "        # –í—ã–±–æ—Ä –ø–µ—Ä–≤—ã—Ö 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        return ' '.join(sentences[:3]) if len(sentences) > 3 else text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Gazeta\n",
        "    train_data = load_gazeta_dataset(split=\"train\", num_samples=5000)\n",
        "    val_data = load_gazeta_dataset(split=\"validation\")\n",
        "\n",
        "    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å large –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ—Å—É—Ä—Å—ã)\n",
        "    summarizer = GazetaSummarizer(model_name=\"csebuetnlp/mT5_m2o_russian_crossSum\")\n",
        "\n",
        "    # 3. –û–±—É—á–µ–Ω–∏–µ\n",
        "    summarizer.train(\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        output_dir=\"./gazeta_summarizer\"\n",
        "    )\n",
        "\n",
        "    # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö\n",
        "    test_data = load_gazeta_dataset(split=\"test\", num_samples=5)\n",
        "    for i, example in enumerate(test_data):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"üì∞ –ü—Ä–∏–º–µ—Ä {i+1}\")\n",
        "        print(\"–û—Ä–∏–≥–∏–Ω–∞–ª:\", example[\"text\"][:200].replace(\"\\n\", \" \") + \"...\")\n",
        "        print(\"–≠—Ç–∞–ª–æ–Ω:\", example[\"summary\"])\n",
        "        print(\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ:\", summarizer.generate_summary(example[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfwuZg84SqQE"
      },
      "source": [
        "–¢–µ—Å—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLNTmArTPZWf"
      },
      "outputs": [],
      "source": [
        "model_path = \"./gazeta_summarizer\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "def summarize(text):\n",
        "    inputs = tokenizer(\n",
        "        f\"summarize: {text}\",\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=600,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=150,\n",
        "            num_beams=5,\n",
        "            do_sample=True,\n",
        "            top_k=60,\n",
        "            top_p=0.9,\n",
        "            temperature=0.8,\n",
        "            repetition_penalty=1.3,\n",
        "            length_penalty=1.2\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD0iVCdqPZT5"
      },
      "outputs": [],
      "source": [
        "texts='–ü–æ—á—Ç–∏ 1,3 —Ç—ã—Å—è—á–∏ –∂–∏—Ç–µ–ª–µ–π –∫—É—Ä—Å–∫–æ–≥–æ –ø—Ä–∏–≥—Ä–∞–Ω–∏—á—å—è, –∫–æ—Ç–æ—Ä—ã–µ —á–∏—Å–ª–∏–ª–∏—Å—å –≤ —Ä–µ–µ—Å—Ç—Ä–µ —É—Ç—Ä–∞—Ç–∏–≤—à–∏—Ö —Å–≤—è–∑—å —Å —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞–º–∏ –∏–∑-–∑–∞ –¥–µ–π—Å—Ç–≤–∏–π –í–°–£, –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç. –û–± —ç—Ç–æ–º —Å–æ–æ–±—â–∏–ª –≤—Ä–∏–æ –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –•–∏–Ω—à—Ç–µ–π–Ω. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ –µ–≥–æ —Å–ª–æ–≤–∞–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –µ—â–µ 421 —á–µ–ª–æ–≤–µ–∫–∞. –ò —Ü–∏—Ñ—Ä–∞ –ª—é–¥–µ–π, —á—å–µ –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–µ–≥–æ–¥–Ω—è, —É–≤—ã, –Ω–∞–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ, - –Ω–∞ –¥–∞–Ω–Ω—É—é –º–∏–Ω—É—Ç—É —ç—Ç–æ 576 —á–µ–ª–æ–≤–µ–∫, - –¥–æ–±–∞–≤–∏–ª –•–∏–Ω—à—Ç–µ–π–Ω, –æ—Ç–º–µ—Ç–∏–≤, —á—Ç–æ —ç—Ç–æ –æ—á–µ–Ω—å –±–æ–ª—å—à–∞—è —Ü–∏—Ñ—Ä–∞, –Ω–æ –æ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–ª–æ–∂–∏–ª–∞—Å—å –∏–∑ —Ä—è–¥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –° –º–æ–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–µ—Å—Ç—Ä–∞ –≤ –Ω–µ–≥–æ –±—ã–ª–æ –≤–Ω–µ—Å–µ–Ω–æ 2287 —á–µ–ª–æ–≤–µ–∫, —É—Ç–æ—á–Ω–∏–ª –≤—Ä–∏–æ –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–∞. –û–Ω —Ç–∞–∫–∂–µ –ø–æ—è—Å–Ω–∏–ª, —á—Ç–æ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–µ–µ—Å—Ç—Ä–∞ —Å–æ–±–∏—Ä–∞–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ –æ—Ç —Ü–µ–ª–æ–≥–æ —Ä—è–¥–∞ –≤–µ–¥–æ–º—Å—Ç–≤. –í–ª–∞—Å—Ç–∏ –≤ —Ç–æ–º —á–∏—Å–ª–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–∏ –≤ –°–§–† –¥–∞–Ω–Ω—ã–µ –æ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å –∞–≤–≥—É—Å—Ç–∞ 2024 –≥–æ–¥–∞ –Ω–µ –ø–æ–ª—É—á–∞–ª–∏ –ø–µ–Ω—Å–∏—é –∏–∑ –ø—Ä–∏–≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Ä–∞–π–æ–Ω–æ–≤.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPgQpbghPZQz"
      },
      "outputs": [],
      "source": [
        "print(summarize(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6UavJaMTI2c"
      },
      "source": [
        "zip –º–æ–¥–µ–ª–∏, –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Å–∫–∞—á–∫–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0VOg64UTOO8"
      },
      "outputs": [],
      "source": [
        "!zip -r gazeta_summarizer.zip ./gazeta_summarizer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
